{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42482946",
   "metadata": {},
   "source": [
    "# Phrase Query → Top-k Panel + CAMs\n",
    "\n",
    "Loads FAISS + ids and lets you query by phrase:\n",
    "- 3×3 panel of top-k (raw, CAM overlay, optional point/box)\n",
    "- Optional 15-sec GIF cycling top-k overlays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a246d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports & setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "repo_root = Path.cwd().resolve().parents[0] if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "sys.path.append(str(repo_root / \"src\"))\n",
    "\n",
    "from pgr import encoders, index, viz, cam\n",
    "from pgr.utils import seed_everything, get_device\n",
    "from pgr_dl import io_deeplesion as io, adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2369636d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config (point to results from 00)\n",
    "RESULTS_DIR = repo_root / \"results\" / \"kaggle_v1\"\n",
    "INDEX_PATH  = RESULTS_DIR / \"index.faiss\"\n",
    "IDS_PATH    = RESULTS_DIR / \"ids.parquet\"\n",
    "MODEL_NAME  = \"ViT-B/16\"\n",
    "PANEL_ROWS, PANEL_COLS = 3, 3\n",
    "K = PANEL_ROWS * PANEL_COLS\n",
    "ALPHA = 0.5  # CAM overlay\n",
    "\n",
    "assert INDEX_PATH.exists() and IDS_PATH.exists(), \"Run 00_build_index_dl.ipynb first.\"\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60181c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load artifacts\n",
    "ids_df = pd.read_parquet(IDS_PATH)\n",
    "fa = index.FaissIndex.load(str(INDEX_PATH))\n",
    "enc = encoders.ClipEncoder(model_name=MODEL_NAME, device=str(get_device(None)))\n",
    "\n",
    "print(f\"IDs: {ids_df.shape}, Index dim: {fa.dim}, Device: {next(enc.model.parameters()).device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525c9c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Query phrase → top-k search\n",
    "phrase = \"liver lesion\"   # <<< change here\n",
    "\n",
    "q_vec = adapters.encode_phrase(enc, phrase).cpu().numpy()   # (1,D) float32, L2-normed\n",
    "scores, I, _ = fa.search(q_vec, k=K)\n",
    "\n",
    "hits = []\n",
    "for rank, idx in enumerate(I[0], start=1):\n",
    "    row = ids_df.iloc[int(idx)]\n",
    "    hits.append({**row.to_dict(), \"rank\": rank, \"score\": float(scores[0, rank-1])})\n",
    "hits_df = pd.DataFrame(hits)\n",
    "hits_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e98985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build 3×3 panel with CAM overlays\n",
    "from pgr.viz import overlay_cam, draw_point_or_box, grid\n",
    "\n",
    "# Wrap the image encoder so CAM sees a module whose forward -> image embeddings\n",
    "import torch.nn as nn\n",
    "class ImageEmbedder(nn.Module):\n",
    "    def __init__(self, clip_model): \n",
    "        super().__init__()\n",
    "        self.clip = clip_model\n",
    "    def forward(self, x): \n",
    "        return self.clip.encode_image(x)\n",
    "\n",
    "img_encoder = ImageEmbedder(enc.model).eval()\n",
    "\n",
    "tiles = []\n",
    "for _, r in hits_df.iterrows():\n",
    "    # base RGB (3-window CT for consistent overlays)\n",
    "    img = io.load_slice(r.img_path)\n",
    "    rgb = adapters.prepare_slice(r, size=224)      # (1,3,H,W) tensor (CLIP-normalized)\n",
    "    # keep a uint8 RGB for overlay background\n",
    "    rgb_uint8 = io.load_slice(r.img_path)\n",
    "    from pgr_dl import windowing\n",
    "    if rgb_uint8.ndim == 2 or rgb_uint8.dtype != np.uint8:\n",
    "        rgb_uint8 = windowing.ct3ch(rgb_uint8)\n",
    "\n",
    "    # CAM\n",
    "    pvec = adapters.encode_phrase(enc, phrase)     # (1,D)\n",
    "    cam_map = cam.gradcam_vit(rgb, pvec, img_encoder)  # (H,W) in [0,1]\n",
    "\n",
    "    # Overlay + optional GT box\n",
    "    over = overlay_cam(rgb_uint8, cam_map, alpha=ALPHA)\n",
    "    gt_box = None\n",
    "    has_box_cols = {\"bbox_x1\",\"bbox_y1\",\"bbox_x2\",\"bbox_y2\"}.issubset(hits_df.columns)\n",
    "    if has_box_cols and pd.notna(r.get(\"bbox_x1\")):\n",
    "        gt_box = (int(r[\"bbox_x1\"]), int(r[\"bbox_y1\"]), int(r[\"bbox_x2\"]), int(r[\"bbox_y2\"]))\n",
    "    over = draw_point_or_box(over, point=None, box=gt_box)\n",
    "    tiles.append(over)\n",
    "\n",
    "panel = grid(tiles, rows=PANEL_ROWS, cols=PANEL_COLS)\n",
    "panel_path = RESULTS_DIR / f\"panel_{phrase.replace(' ','_')}.png\"\n",
    "panel.save(panel_path)\n",
    "panel_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de478c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (Optional) 15-sec GIF\n",
    "from pgr.viz import save_gif\n",
    "\n",
    "gif_frames = tiles  # already PIL Images\n",
    "gif_path = RESULTS_DIR / f\"demo_{phrase.replace(' ','_')}.gif\"\n",
    "save_gif(gif_frames, str(gif_path), fps=max(1, int(len(gif_frames) / 15.0)))  # ~15s total\n",
    "gif_path\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
